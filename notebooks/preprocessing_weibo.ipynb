{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"data/weibo_preprocessed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uidlist.txt, repost_idlist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_to_userids : \n",
      "|   uid |   userid |\n",
      "|------:|---------:|\n",
      "|     0 |    10029 |\n",
      "|     1 |    10057 |\n",
      "shape : (1787443, 1)\n",
      "\n",
      "cascade_to_mids : \n",
      "|   cascade |         mid |\n",
      "|----------:|------------:|\n",
      "|         0 | 3.50952e+15 |\n",
      "|         1 | 3.49137e+15 |\n",
      "shape : (300000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "u_to_userids = pd.read_csv(\"data/weibo/weibodata/diffusion/uidlist.txt\",header=None)\n",
    "u_to_userids.columns = ['userid']\n",
    "u_to_userids.index.name = 'uid'\n",
    "\n",
    "cascade_to_mids = pd.read_csv(\"data/weibo/weibodata/diffusion/repost_idlist.txt\", header=None,)\n",
    "cascade_to_mids.columns = ['mid']\n",
    "cascade_to_mids.index.name = 'cascade'\n",
    "cascade_to_mids.mid = cascade_to_mids.mid.astype(np.int64)\n",
    "\n",
    "print(\"u_to_userids : \\n\" + u_to_userids.head(2).to_markdown())\n",
    "print(f\"shape : {u_to_userids.shape}\\n\")\n",
    "print(\"cascade_to_mids : \\n\" + cascade_to_mids.head(2).to_markdown())\n",
    "print(f\"shape : {cascade_to_mids.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract : \n",
    "\n",
    "     id\n",
    "     bi_followers_count\n",
    "     city\n",
    "     verified\n",
    "     followers_count\n",
    "     location\n",
    "     province\n",
    "     friends_count\n",
    "     name\n",
    "     gender\n",
    "     created_at\n",
    "     verified_type\n",
    "     statuses_count\n",
    "     description\n",
    "    \n",
    "     1657151084\n",
    "     0\n",
    "     5\n",
    "     False\n",
    "     33\n",
    "     �Ϻ� ������\n",
    "     31\n",
    "     162\n",
    "     JACKJONES\n",
    "     m\n",
    "     2009-10-29-22:20:41\n",
    "     -1\n",
    "     0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "user_profile : \n",
      "uid                      int64\n",
      "bi_followers_count       int32\n",
      "city                  category\n",
      "verified              category\n",
      "followers_count          int32\n",
      "location                object\n",
      "province              category\n",
      "friends_count            int32\n",
      "name                    object\n",
      "gender                category\n",
      "created_at              object\n",
      "verified_type         category\n",
      "statuses_count           int32\n",
      "description             object\n",
      "dtype: object\n",
      "shape : (1681085, 14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creates the dataframe of user data and saves it in userProfile.pkl\n",
    "\"\"\"\n",
    "def convert_userProfile(save) : \n",
    "    \n",
    "    df = [[] for _ in range(15)]\n",
    "    with open(\"data/weibo/weibodata/userProfile/user_profile1.txt\", mode='r', encoding='gbk') as file : \n",
    "        c = 0\n",
    "        for line in file : \n",
    "            if c >= 15 : #not adding the first 15 lines : headers\n",
    "                df[c%15].append(line.replace('\\n',''))\n",
    "            c += 1\n",
    "    \n",
    "    with open(\"data/weibo/weibodata/userProfile/user_profile2.txt\", mode='r', encoding='gbk') as file : \n",
    "        c = 0\n",
    "        for line in file : \n",
    "            if c >= 15 :\n",
    "                df[c%15].append(line.replace('\\n',''))\n",
    "            c += 1\n",
    "    d = {}\n",
    "    for i in range(14) :\n",
    "        d[i] = df[i]\n",
    "    df = pd.DataFrame.from_dict(d, orient='columns')\n",
    "    df.columns = ['uid', 'bi_followers_count', 'city', \n",
    "                 'verified', 'followers_count', 'location', \n",
    "                 'province', 'friends_count', 'name', \n",
    "                 'gender', 'created_at', 'verified_type', \n",
    "                 'statuses_count', 'description']\n",
    "    df = df.astype({    'uid':np.int64, \n",
    "                        'bi_followers_count':np.int32, \n",
    "                        'city':'category', \n",
    "                        'verified':'category', \n",
    "                        'followers_count':np.int32, \n",
    "                        'province':'category', \n",
    "                        'friends_count':np.int32, \n",
    "                        'gender':'category', \n",
    "                        'verified_type':'category', \n",
    "                        'statuses_count': np.int32})\n",
    "    print(f\"\\nuser_profile : \\n{df.dtypes}\")\n",
    "    print(f\"shape : {df.shape}\\n\")\n",
    "    \n",
    "    if save : \n",
    "        df.to_pickle(output_folder + \"userProfile.pkl\")\n",
    "    return df\n",
    "    \n",
    "df = convert_userProfile(True)\n",
    "\n",
    "#user_profile = pd.read_pickle(output_folder + \"userProfile.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract : \n",
    "\n",
    "    3479380645050771 2012-08-15-20:07:32 1500872 3\n",
    "    1500872 2012-08-15-20:41:02 1500872 2012-08-15-20:41:20\n",
    "    3464585309142627 2012-07-06-00:16:10 153602 595\n",
    "    1598909 2012-07-06-00:17:29 1727898 2012-07-06-00:24:20 1460582 2012-07-06-00:25:07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create infos_cascades/user_cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_cascades(save = False) :\n",
    "    \"\"\"\n",
    "    creates 2 files with information about all the cascades : \n",
    "        - infos_cascades : \n",
    "            i:n_cascades - mid(int64) - date(pd.DateTime) - u(int32) - n_likes(int32) - n_reposts(int32) - users2(list(int32))\n",
    "        - user_cascades : \n",
    "            i:v(int32) - mids(list(int64)) - Av(int32)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    d_user_cascade = defaultdict(lambda : [[]]) # d_user_cascade[userid] = list of cascades in which user appears\n",
    "    infos_cascades = []\n",
    "\n",
    "    with open(\"data/weibo/weibodata/total.txt\", 'r') as file :\n",
    "        count_line = 0 \n",
    "        mid_cascade = None\n",
    "\n",
    "        for line in file:\n",
    "        \n",
    "            if count_line % 60000 == 0 : print(f\"{count_line//2000}K cascades processed\")\n",
    "        \n",
    "            if count_line%2 == 0 : #mid - date - user1 - #likes\n",
    "                mid_cascade = np.int64(line.replace('\\n', '').split()[0])\n",
    "                infos_cascades.append(line.replace('\\n', '').split())\n",
    "        \n",
    "            else : # (uid timestamp )*\n",
    "                curr_cascade = line.replace('\\n', '').split()\n",
    "                users2 = []\n",
    "                for i in range(0, len(curr_cascade), 2) :\n",
    "                    u = np.int32(curr_cascade[i])\n",
    "                    d_user_cascade[u][0].append(mid_cascade)\n",
    "                    users2.append(u)\n",
    "                    \n",
    "                infos_cascades[-1].append(len(curr_cascade)//2) #adds the number of reposts\n",
    "                infos_cascades[-1].append(users2) \n",
    "                \n",
    "            count_line+= 1\n",
    "    print(f\"{count_line//2000}K cascades processed\")\n",
    "    user_cascades = pd.DataFrame.from_dict(d_user_cascade, orient='index', columns = ['mids'])\n",
    "    user_cascades['#cascades'] = user_cascades.mids.apply(len)\n",
    "    \n",
    "    print(f\"\\nuser_cascades : \\n{user_cascades.dtypes}\")\n",
    "    print(f\"shape : {user_cascades.shape}\\n\")\n",
    "    \n",
    "    infos_cascades = pd.DataFrame(infos_cascades, columns = ['mid', 'date', 'User1', '#likes', '#reposts', 'users2'],)\n",
    "    infos_cascades = infos_cascades.astype(dtype={'mid' : np.int64, 'User1' : np.int32, '#likes' : np.int32, '#reposts' : np.int32})\n",
    "    infos_cascades['date'] = pd.to_datetime(infos_cascades.date)\n",
    "\n",
    "    print(f\"\\ninfos_cascades : \\n{infos_cascades.dtypes}\" )\n",
    "    print(f\"shape : {infos_cascades.shape}\\n\")\n",
    "\n",
    "    if save:\n",
    "        user_cascades.to_pickle(output_folder + \"user_cascades.pkl\")\n",
    "        infos_cascades.to_pickle(output_folder + \"infos_cascades.pkl\")\n",
    "\n",
    "    return user_cascades, infos_cascades\n",
    "\n",
    "#user_cascades, infos_cascades = extract_info_cascades(True)\n",
    "\n",
    "user_cascades = pd.read_pickle(output_folder + \"user_cascades.pkl\")\n",
    "infos_cascades = pd.read_pickle(output_folder + \"infos_cascades.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsampling 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling_1(n_cascades, n_influences_max):\n",
    "    \"\"\"\n",
    "    Only extracts the n_cascades first cascades and only the n_influences_max first reposts of each cascade\n",
    "    \n",
    "    \"\"\"\n",
    "    edges = pd.DataFrame.copy(infos_cascades.iloc[:n_cascades][['User1','users2']])\n",
    "    edges.users2 = edges.users2.apply(lambda x : x[:n_influences_max])\n",
    "    edges = edges.explode('users2')\n",
    "    edges.columns = ['u','v']\n",
    "\n",
    "    Au = edges.groupby('u').count().v\n",
    "    Au.name = 'Au'\n",
    "    Av = edges.groupby('v').count().u\n",
    "    Av.name = 'Av'\n",
    "    edges = edges.merge(Au, on='u')\n",
    "    edges = edges.merge(Av, on='v')\n",
    "\n",
    "    Au2v = edges.groupby(['u', 'v']).count().Au.reset_index()\n",
    "    edges = edges.merge(Au2v, on=['u','v'])\n",
    "    \n",
    "    edges.columns = ['u', 'v', 'Au', 'Av', 'Au2v']\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsampling 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling_2(n_cascades_max, n_min_reposts) : \n",
    "    \"\"\"\n",
    "    Only selects edges between influencers and targets having more than n_min_reposts globally\n",
    "    \"\"\"\n",
    "    targets = list(user_cascades[user_cascades['#cascades'] > n_min_reposts].index)\n",
    "    d_targets = defaultdict(lambda : False)\n",
    "    for t in targets : \n",
    "        d_targets[t] = True\n",
    "\n",
    "    edges = pd.DataFrame.copy(infos_cascades.iloc[:n_cascades_max][['User1','users2']])\n",
    "    edges.users2 = edges.users2.apply(lambda vs : [v for v in vs if d_targets[v]])\n",
    "    \n",
    "    edges = edges.explode('users2')\n",
    "    edges.columns = ['u','v']\n",
    "\n",
    "    Au = edges.groupby('u').count().v\n",
    "    Au.name = 'Au'\n",
    "    Av = edges.groupby('v').count().u\n",
    "    Av.name = 'Av'\n",
    "\n",
    "    edges = edges.merge(Au, on='u')\n",
    "    edges = edges.merge(Av, on='v')\n",
    "\n",
    "    Au2v = edges.groupby(['u', 'v']).count().Au.reset_index()\n",
    "    edges = edges.merge(Au2v, on=['u','v'])\n",
    "    \n",
    "    edges.columns = ['u', 'v', 'Au', 'Av', 'Au2v']\n",
    "    edges = edges.drop_duplicates()\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsampling 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling_3():\n",
    "    \"\"\"\n",
    "    Selects randomly X targets from the cascade, with higher chance for short reaction time (tu-tv)^-1/sum(tu-tv)^-1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(edges, save=False, file_name=\"\") : \n",
    "    \"\"\"\n",
    "    input : edges dataframe \n",
    "    \"\"\"\n",
    "    edges['BT'] = edges['Au2v'] / edges['Au']\n",
    "    edges['JI'] = edges['Au2v'] / (edges['Au'] + edges['Av'])\n",
    "    edges['LP'] = edges['Au2v'] / edges['Av']\n",
    "\n",
    "    edges = edges.merge(u_to_userids, left_on='u', right_on = 'uid',)\n",
    "    edges = edges.merge(u_to_userids, left_on='v', right_on = 'uid',)\n",
    "    edges['u'] = edges['userid_x']\n",
    "    edges['v'] = edges['userid_y']\n",
    "\n",
    "\n",
    "    edges = edges[['u', 'v', 'BT', 'JI', 'LP']]\n",
    "    edges = edges.drop_duplicates()\n",
    "    print(\"edges_probabilities : \\n\" + edges.head(5).to_markdown())\n",
    "    print(f\"shape : {edges.shape}\\n\")\n",
    "\n",
    "    if save :\n",
    "        print(\"Saved \" + file_name)\n",
    "        pd.to_pickle(edges, output_folder+file_name)\n",
    "    \n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39529, 5) 3156 36410\n",
      "edges_probabilities : \n",
      "|    |           u |           v |        BT |        JI |   LP |\n",
      "|---:|------------:|------------:|----------:|----------:|-----:|\n",
      "|  0 | 1.67892e+09 | 1.67892e+09 | 1         | 0.5       |  1   |\n",
      "|  3 | 2.30505e+09 | 1.8462e+09  | 0.025     | 0.0238095 |  0.5 |\n",
      "|  4 | 1.8938e+09  | 1.8462e+09  | 0.0196078 | 0.0188679 |  0.5 |\n",
      "|  5 | 2.30505e+09 | 2.17002e+09 | 0.025     | 0.0243902 |  1   |\n",
      "|  6 | 2.30505e+09 | 1.4391e+09  | 0.025     | 0.0243902 |  1   |\n",
      "shape : (37918, 5)\n",
      "\n",
      "Saved labels1_5K_10.pkl\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "\n",
    "# edges = subsampling_2(-1, n)\n",
    "edges = subsampling_1(5000, 10)\n",
    "i = len(edges.groupby('u').count().index)\n",
    "t = len(edges.groupby('v').count().index)\n",
    "print(edges.shape, i, t)\n",
    "labels = estimate_probabilities(edges, True, f\"labels1_5K_10.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 100 | (1839405, 5) 27626 22297\n",
    "- 150 | (380010, 5) 14280 3998\n",
    "- 200 | (111575, 5) 8000 1161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract : \n",
    "\n",
    "    #doc name topic proportion ...\n",
    "    0\t3515638699605834\t70\t0.025423728813559324\t52\t0.025423728813559324 ...\n",
    "    1\t3421815211220296\t7\t0.0873015873015873\t84\t0.03968253968253968\t87\t ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create topic.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_file(save=False):\n",
    "    \n",
    "    d = {'mid' : []}\n",
    "    for i in range(100) :\n",
    "        d[i] = []\n",
    "\n",
    "    with open('data/weibo/weibodata/topic-100/doc', 'r') as file : \n",
    "        i = 0\n",
    "        for line in file :\n",
    "            line = line.split('\\t')\n",
    "            if len(line) < 2 : continue\n",
    "            \n",
    "            d['mid'].append(np.int64(line[1]))\n",
    "            for t in range(100) :\n",
    "                d[int(line[2*t+2])].append(float(line[2*t+3]))\n",
    "\n",
    "            i += 1\n",
    "        df_topic = pd.DataFrame.from_dict(d)\n",
    "        df_topic.index = df_topic.mid\n",
    "        df_topic = df_topic.drop(columns='mid')\n",
    "        del(d)\n",
    "    \n",
    "    print(\"df_topics : \\n\")\n",
    "    print(f\"shape : {df_topic.shape}\\n\")\n",
    "    \n",
    "    if save : \n",
    "        df_topic.to_pickle(output_folder + \"topic.pkl\")       \n",
    "    return df_topic\n",
    "\n",
    "#df_topic = create_topic_file(True)\n",
    "df_topic = pd.read_pickle(output_folder + \"topic.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering  new topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as spc\n",
    "N_TOPICS = 8\n",
    "\n",
    "def convert_new_topic(df_topic, save=False) : \n",
    "    \"\"\"\n",
    "    input : df_topic \n",
    "    output : df with N_TOPICS columns \n",
    "    \"\"\"\n",
    "    df_topic_sample = df_topic.sample(n = 10000)\n",
    "    corr = df_topic_sample.corr().values\n",
    "\n",
    "    pdist = spc.distance.pdist(corr, metric='euclidean')\n",
    "    linkage = spc.linkage(pdist, method='complete')\n",
    "    idx = spc.fcluster(linkage, N_TOPICS, 'maxclust')\n",
    "\n",
    "    df_new = pd.DataFrame(np.zeros((df_topic.shape[0], N_TOPICS)), columns=[str(k) for k in range(N_TOPICS)])\n",
    "    df_new.index = df_topic.index\n",
    "    for i in range(100) : \n",
    "        new_i = idx[i] -1\n",
    "        df_new[str(new_i)] += df_topic[i]\n",
    "    \n",
    "    if save : \n",
    "        df_new.to_pickle(output_folder + f\"topics_{N_TOPICS}.pkl\")\n",
    "    return df_new\n",
    "\n",
    "def convert_new_topic2(df_topic, save=False):\n",
    "    LOW = [0, 49, 26, 13, 47, 73, 97, 80, 66, 55, 17, 68, 58, 59, 8, 92, 65, 15, 70, 61]\n",
    "    MID = [45, 54, 85, 24, 96, 22, 32, 40, 77, 4, 74, 67, 41, 79, 71, 60, 95, 28, 38, 33, 20, 81, 63, 46, 27, 52, 34, 94, 18, 16, 53, 9, 10, 50, 91, 89, 48, 25, 19, 43, 93, 82, 83, 31, 30, 56, 69, 29, 88, 1, 84, 51, 12, 11, 62, 57, 3, 37, 78, 76]\n",
    "    HIGH = [86, 64, 98, 35, 23, 72, 44, 21, 75, 90, 5, 87, 6, 2, 39, 14, 7, 99, 42, 36]\n",
    "    df_new = pd.DataFrame(np.zeros((df_topic.shape[0], 3)), columns=['LowT', 'MidT', 'HighT'])\n",
    "    df_new.index = df_topic.index\n",
    "    for i in range(100) : \n",
    "        if i in LOW : \n",
    "            df_new['LowT'] += df_topic[i]\n",
    "        elif i in MID :\n",
    "            df_new['MidT'] += df_topic[i]\n",
    "        elif i in HIGH : \n",
    "            df_new['HighT'] += df_topic[i]\n",
    "    if save : \n",
    "        df_new.to_pickle(output_folder + f\"topics_infl.pkl\")\n",
    "    return df_new\n",
    "\n",
    "df_new_topic = convert_new_topic2(df_topic, True)\n",
    "#df_new_topic = pd.read_pickle(output_folder + f\"topics_{N_TOPICS}.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the average topic representation of a list of mids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 3\n",
    "\n",
    "d_new_topic = defaultdict(lambda : 1/N_TOPICS * np.ones(N_TOPICS))\n",
    "for mid in df_new_topic.index : \n",
    "    d_new_topic[mid] = np.array(df_new_topic.loc[mid])\n",
    "\n",
    "def create_topic_vector(mids) :\n",
    "    \"\"\"\n",
    "    given a list of mids, estimates the average topic representation \n",
    "    \"\"\"\n",
    "    n = len(mids)\n",
    "    s = np.zeros((3,))\n",
    "    for mid in mids :\n",
    "        s += np.array(d_new_topic[mid])\n",
    "    return 1/n * s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topic per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infos_targets : \n",
      "|     userid |        0 |        1 |        2 |\n",
      "|-----------:|---------:|---------:|---------:|\n",
      "| 1678921627 | 0.199416 | 0.569909 | 0.230674 |\n",
      "| 1846200282 | 0.177129 | 0.59947  | 0.223402 |\n",
      "shape : (1334887, 3)\n",
      "\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_topic_per_target(save=False):\n",
    "    \"\"\"\n",
    "    creates a df linking the uids of targets to the average topic representation of the reposted posts\n",
    "    input : user_cascades\n",
    "    \"\"\"\n",
    "    infos_targets = pd.DataFrame.copy(user_cascades[['mids']])\n",
    "    infos_targets = infos_targets.merge(u_to_userids, left_on=infos_targets.index, right_on=u_to_userids.index)[['mids','userid']]\n",
    "    \n",
    "    \n",
    "    # topics_users = infos_targets.mids.apply(create_topic_vector)\n",
    "    # topics_users = pd.DataFrame(topics_users.to_list(), index = topics_users.index,)\n",
    "    # topics_users.to_pickle(output_folder + \"backup_topics_users_3.pkl\")\n",
    "    topics_users = pd.read_pickle(output_folder + \"backup_topics_users.pkl\")\n",
    "    \n",
    "    infos_targets = pd.concat([infos_targets, topics_users], axis=1)\n",
    "    infos_targets = infos_targets.drop(columns=\"mids\")\n",
    "    infos_targets = infos_targets.set_index('userid')\n",
    "\n",
    "    print(\"infos_targets : \\n\" + infos_targets.head(2).to_markdown())\n",
    "    print(f\"shape : {infos_targets.shape}\\n\")\n",
    "\n",
    "    if save :\n",
    "        print('Saving...')\n",
    "        infos_targets.to_pickle(output_folder + \"infos_targets_3.pkl\")\n",
    "\n",
    "    return infos_targets\n",
    "\n",
    "infos_targets = create_topic_per_target(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topic per influencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infos_influencers : \n",
      "|   userid |   total_likes |   total_reposts |   n_cascades |        0 |        1 |        2 |\n",
      "|---------:|--------------:|----------------:|-------------:|---------:|---------:|---------:|\n",
      "|    10029 |            10 |               5 |            1 | 0.222222 | 0.619048 | 0.15873  |\n",
      "|    10111 |             2 |               1 |            1 | 0.183333 | 0.633333 | 0.183333 |\n",
      "shape : (47555, 6)\n",
      "\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "def create_topic_per_influencer(save=False):\n",
    "    \"\"\"\n",
    "    creates a df linking the uids of influencers to the average topic representation of the created posts\n",
    "    \"\"\"\n",
    "    infos_influencers = infos_cascades.groupby('User1').agg({'mid' : list, \n",
    "                                                            '#likes' : 'sum', \n",
    "                                                            '#reposts' : 'sum', \n",
    "                                                            'users2' : 'count'})\n",
    "    infos_influencers = infos_influencers.merge(u_to_userids, left_on = infos_influencers.index, right_on=u_to_userids.index)\n",
    "    infos_influencers = infos_influencers.drop(columns='key_0')\n",
    "    infos_influencers = infos_influencers.set_index('userid')\n",
    "    infos_influencers.columns = ['mids','total_likes', 'total_reposts', 'n_cascades']\n",
    "\n",
    "    df2 = infos_influencers.mids.apply(create_topic_vector) \n",
    "    df2 = pd.DataFrame(df2.to_list(), index = df2.index,)\n",
    "    \n",
    "    infos_influencers = pd.concat([infos_influencers, df2], axis=1)\n",
    "    infos_influencers = infos_influencers.drop(columns=\"mids\")\n",
    "\n",
    "    print(\"infos_influencers : \\n\" + infos_influencers.head(2).to_markdown())\n",
    "    print(f\"shape : {infos_influencers.shape}\\n\")\n",
    "\n",
    "    if save :\n",
    "        print(\"Saving...\")\n",
    "        infos_influencers.to_pickle(output_folder + \"infos_influencers_3.pkl\")\n",
    "\n",
    "    return infos_influencers\n",
    "\n",
    "infos_influencers = create_topic_per_influencer(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph_170w_1month.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract : \n",
    "\n",
    "    0 1 1\n",
    "    0 2 1\n",
    "    0 15 1\n",
    "    ...\n",
    "    1 5 1\n",
    "    1 14 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I = 3156, T = 36410\n",
      "users_topology : must be <1.8M \n",
      " [1785013, 1785560, 1785599, 1786130, 1786162, 1786321, 1786597, 1786755, 1786763, 1786764] ...\n",
      "length : 39098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "influencers = list(edges.groupby('u').count().index) #with uid\n",
    "targets = list(edges.groupby('v').count().index)\n",
    "\n",
    "users_topology = list(set(influencers + targets))\n",
    "users_topology = sorted(users_topology)\n",
    "\n",
    "print(f\"I = {len(influencers)}, T = {len(targets)}\")\n",
    "print(f\"users_topology : must be <1.8M \\n {users_topology[-10:]} ...\" )\n",
    "print(f\"length : {len(users_topology)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17M lines processed : 132899 edges added\n",
      "34M lines processed : 237261 edges added\n",
      "51M lines processed : 280877 edges added\n",
      "68M lines processed : 394408 edges added\n",
      "85M lines processed : 513114 edges added\n",
      "102M lines processed : 602082 edges added\n",
      "119M lines processed : 724361 edges added\n",
      "136M lines processed : 837217 edges added\n",
      "153M lines processed : 956586 edges added\n",
      "170M lines processed : 1039668 edges added\n",
      "187M lines processed : 1097541 edges added\n",
      "204M lines processed : 1193765 edges added\n",
      "221M lines processed : 1307547 edges added\n",
      "238M lines processed : 1442467 edges added\n",
      "255M lines processed : 1550755 edges added\n",
      "272M lines processed : 1625779 edges added\n",
      "289M lines processed : 1682062 edges added\n",
      "306M lines processed : 1794122 edges added\n",
      "323M lines processed : 1870592 edges added\n",
      "340M lines processed : 1910907 edges added\n",
      "357M lines processed : 1994332 edges added\n",
      "374M lines processed : 2123575 edges added\n",
      "391M lines processed : 2225076 edges added\n",
      "408M lines processed : 2297879 edges added\n",
      "Saved edges1_5K_10.pkl\n"
     ]
    }
   ],
   "source": [
    "def create_edges(users_topology) : \n",
    "    \"\"\"\n",
    "    Creates weibo/edges_NB_CASCADES.pkl ---> |id|u|v|\n",
    "    2.4M edges for 1000 cascades\n",
    "    Takes 4min to extract\n",
    "    \"\"\"\n",
    "    \n",
    "    #Speeds up operation (v in users)\n",
    "    d_users = defaultdict(lambda : False)\n",
    "    for user in users_topology:\n",
    "        d_users[int(user)] = True\n",
    "\n",
    "    edges_topology = []                 \n",
    "\n",
    "    with open(\"data/weibo/weibodata/graph_170w_1month.txt\", 'r') as file :\n",
    "        \n",
    "        u_previous = -1\n",
    "        u_in_table = False\n",
    "\n",
    "        n_lines = 0\n",
    "        \n",
    "        for line in file :\n",
    "            \n",
    "            n_lines += 1\n",
    "            if n_lines % 17000000 == 0 :\n",
    "                print(f\"{n_lines//1000000}M lines processed : {len(edges_topology)} edges added\")\n",
    "\n",
    "            line = line.split(' ')\n",
    "            if len(line) >= 2 :\n",
    "                u,v = int(line[0]), int(line[1])\n",
    "\n",
    "                if u != u_previous : \n",
    "                    u_previous = u\n",
    "                    u_in_table = (d_users[u])\n",
    "\n",
    "                if u_in_table: \n",
    "                    if d_users[v] : \n",
    "                        edges_topology.append((u,v))\n",
    "             \n",
    "    return edges_topology\n",
    "\n",
    "def process_edges(edges_topology, save = False, file_name = \"\"):\n",
    "    \"\"\"\n",
    "    replaces user id with the real ones \n",
    "    \"\"\"\n",
    "    df_edges = pd.DataFrame(edges_topology, columns=['u','v'])\n",
    "    df_edges = df_edges.merge(u_to_userids, how='inner', left_on='u', right_on='uid')\\\n",
    "                        .merge(u_to_userids, how='inner', left_on='v', right_on='uid')\\\n",
    "                        .drop(columns = ['u','v'])\n",
    "    df_edges = df_edges.rename(columns = { 'userid_y' : 'u', 'userid_x' : 'v'})\n",
    "    \n",
    "    if save :\n",
    "        print(f\"Saved {file_name}\")\n",
    "        df_edges.to_pickle(output_folder + file_name)\n",
    "    \n",
    "    return df_edges\n",
    "\n",
    "edges_topology = create_edges(users_topology)\n",
    "df_edges = process_edges(edges_topology, True, f\"edges1_5K_10.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMINFECTOR embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : \n",
    "\n",
    "    378466:[[-0.33543903 -0.34327558 -0.99156916  0.17543791 -1.1597859   0.62841564\n",
    "    1.0802213  -1.1643004   0.17708263 -1.5982181   0.08002795  1.439778\n",
    "    -0.18871987  0.07738146 -0.40249282 -1.4272877   1.0847903   0.7083911\n",
    "    0.11897249 -0.82501847  0.07072636 -0.8887081  -0.90862906  0.5801698\n",
    "    -0.04055832  0.7963939   0.7347262   0.63257265  0.88813674  0.01559831\n",
    "    0.38137203  0.20726131  0.08162224  0.33317327  0.48780873  0.83368325\n",
    "    -0.5089358  -0.18972501 -0.50727546 -0.5938133  -1.3149095  -0.5530932\n",
    "    -0.20405641  0.5140792  -1.1838571   0.00181923 -0.67174435 -0.54573345\n",
    "    0.3464684   1.2554152 ]]\n",
    "    1583072 : ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target embedding -- code from Yuting\n",
    "import re \n",
    "\n",
    "def extract_embeddings(file_path) : \n",
    "    \"\"\"\n",
    "    creates dict target2emb and influencer2emb\n",
    "    \"\"\"\n",
    "\n",
    "    match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?')\n",
    "    d_emb = {}\n",
    "    emb = ''\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file :\n",
    "            emb += line\n",
    "            if line.endswith(']]\\n'):\n",
    "                l_temp = re.findall(match_number, emb)\n",
    "                assert len(l_temp)==51\n",
    "                number = [float(x) for x in l_temp[1:]]\n",
    "                d_emb[l_temp[0]] =  number\n",
    "                emb = '' \n",
    "\n",
    "    print(\"Embeddings extracted.\")\n",
    "    return d_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings extracted.\n",
      "Embeddings extracted.\n",
      "shape : (1170688, 50)\n",
      "shape : (1170688, 50)\n"
     ]
    }
   ],
   "source": [
    "def process_embeddings(d, save=False, filename = \"\") : \n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    \n",
    "    df.index = df.index.astype(np.int64)\n",
    "    df = df.merge(u_to_userids, left_on = df.index, right_on=u_to_userids.index)\n",
    "    df.index = df.userid\n",
    "    df = df.drop(columns=['key_0', 'userid'])\n",
    "\n",
    "    print(\"shape : \" + str(df.shape))\n",
    "    if save : \n",
    "        df.to_pickle(output_folder + filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "target2emb = extract_embeddings(\"papers\\IMINFECTOR\\data\\Embeddings\\inf2vec_target_embeddings_7.txt\")\n",
    "influencer2emb = extract_embeddings(\"papers\\IMINFECTOR\\data\\Embeddings\\inf2vec_source_embeddings_7.txt\")\n",
    "target2emb = process_embeddings(target2emb, True, \"target_inf2vec.pkl\")\n",
    "influencer2emb = process_embeddings(influencer2emb, True, \"influencers_inf2vec.pkl\")\n",
    "\n",
    "# target2emb = extract_embeddings(\"data\\weibo\\weibo_embedding\\mtl_n_target_embeddings_p.txt\")\n",
    "# influencer2emb = extract_embeddings(\"data\\weibo\\weibo_embedding\\mtl_n_source_embeddings_p.txt\")\n",
    "# target2emb = process_embeddings(target2emb, True, \"target_embeddings.pkl\")\n",
    "# influencer2emb = process_embeddings(influencer2emb, True, \"influencers_embeddings.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsampling_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_n = [50,75,100,125,150,200,250,300]\n",
    "for n_min_repost in tests_n :\n",
    "    edges2 = subsampling_2(10000, n_min_repost)\n",
    "    print(f\"{n_min_repost}, {edges2.shape[0]}, I = {edges2.groupby('u').count().shape[0]}, T = {edges2.groupby('v').count().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    100K cascades                       10K cascades\n",
    "    50, 4533008, I = 24870, T = 179760  50, 559835, I = 4898, T = 168815\n",
    "    75, 1983886, I = 21144, T = 61029   75, 252581, I = 4282, T = 59253\n",
    "    100, 856969, I = 16573, T = 22268   100, 112370, I = 3428, T = 21682\n",
    "    125, 377285, I = 12030, T = 8830    125, 50754, I = 2465, T = 8519\n",
    "    150, 179932, I = 8469, T = 3976     150, 24808, I = 1747, T = 3767\n",
    "    200, 53679, I = 4639, T = 1147      200, 7757, I = 908, T = 1031\n",
    "    250, 21460, I = 3045, T = 467       250, 3218, I = 589, T = 403\n",
    "    300, 7497, I = 1915, T = 203        300, 1115, I = 353, T = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#edges = subsampling_2(100000, 150)\n",
    "#estimate_probabilities(edges)\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize = (15,3), sharey= True)\n",
    "sns.histplot(labels.BT.apply(np.log), bins=10, ax = ax[0])\n",
    "ax[0].set_title(\"BT distribution\")\n",
    "sns.histplot(labels.JI.apply(np.log), bins=10, ax = ax[1])\n",
    "ax[1].set_title(\"JI distribution\")\n",
    "sns.histplot(labels.LP.apply(np.log), bins=10, ax = ax[2])\n",
    "ax[2].set_title(\"LP distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('greedy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e383b0081ac07cdf99c82edde9d4d37193c485cac9bae96a2b2d84a49f1aefca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
