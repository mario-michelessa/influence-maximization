{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_influencers_path = \"data/weibo_features/features_influencers_all_200_twitter.pkl\"\n",
    "features_targets_path = \"data/weibo_features/features_targets_all_200_twitter.pkl\"\n",
    "labels_path = \"data/weibo_preprocessed/labels2_all_200.pkl\"\n",
    "edges_path = \"data/weibo_preprocessed/edges2_all_200.pkl\"\n",
    "influencers_embeddings_path = \"data/weibo_preprocessed/influencers_embeddings.pkl\"\n",
    "targets_embeddings_path = \"data/weibo_preprocessed/target_embeddings.pkl\"\n",
    "influencers_inf2vec_path = \"data/weibo_preprocessed/influencers_inf2vec.pkl\"\n",
    "targets_inf2vec_path = \"data/weibo_preprocessed/target_inf2vec.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_influencers : \n",
      "d_out       float64\n",
      "pagerank    float64\n",
      "#2r         float64\n",
      "dtype: object\n",
      "shape : (7907, 3)\n",
      "features_targets : \n",
      "d_in        float64\n",
      "pagerank    float64\n",
      "dtype: object\n",
      "shape : (1160, 2)\n"
     ]
    }
   ],
   "source": [
    "features_influencers = pd.read_pickle(features_influencers_path)\n",
    "features_targets = pd.read_pickle(features_targets_path)\n",
    "print(\"features_influencers : \\n\" + str(features_influencers.dtypes))\n",
    "print(\"shape : \" + str(features_influencers.shape))\n",
    "print(\"features_targets : \\n\" + str(features_targets.dtypes))\n",
    "print(\"shape : \" + str(features_targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                      |      u |                v |   BT |         JI |         LP |\n",
      "|:---------------------|-------:|-----------------:|-----:|-----------:|-----------:|\n",
      "| (101713, 101713)     | 101713 | 101713           |    1 | 0.0136364  | 0.0138249  |\n",
      "| (104881, 1706372734) | 104881 |      1.70637e+09 |    1 | 0.00340136 | 0.00341297 |\n",
      "shape : (111575, 5)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_pickle(labels_path)\n",
    "labels.index = pd.MultiIndex.from_tuples(zip(labels['u'],labels['v'])) #important to do .loc[(u,v)]\n",
    "labels = labels.sort_index() # infos are retreived faster\n",
    "labels = labels.drop_duplicates()\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))\n",
    "\n",
    "influencers_embeddings = pd.read_pickle(influencers_embeddings_path)\n",
    "targets_embeddings = pd.read_pickle(targets_embeddings_path)\n",
    "# influencers_embeddings = pd.read_pickle(influencers_inf2vec_path)\n",
    "# targets_embeddings = pd.read_pickle(targets_inf2vec_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                      |      u |                v |   BT |         JI |         LP |\n",
      "|:---------------------|-------:|-----------------:|-----:|-----------:|-----------:|\n",
      "| (101713, 101713)     | 101713 | 101713           |    1 | 0.0136364  | 0.0138249  |\n",
      "| (104881, 1706372734) | 104881 |      1.70637e+09 |    1 | 0.00340136 | 0.00341297 |\n",
      "shape : (99436, 5)\n",
      "influencers : 5977\n",
      "targets : 1134\n"
     ]
    }
   ],
   "source": [
    "# removing the labels where we do not have the embeddings\n",
    "# and removing the labels where we do not have the features\n",
    "\n",
    "d_influencers = defaultdict(lambda : 0)\n",
    "for i in list(influencers_embeddings.index) : d_influencers[i] += 1\n",
    "for i in list(features_influencers.index) : d_influencers[i] += 1\n",
    "\n",
    "d_targets = defaultdict(lambda : 0)\n",
    "for i in list(targets_embeddings.index) : d_targets[i] += 1\n",
    "for i in list(features_targets.index) : d_targets[i] += 1\n",
    "\n",
    "labels = labels.drop(labels[labels.u.apply(lambda x : d_influencers[x] < 2)].index)\n",
    "labels = labels.drop(labels[labels.v.apply(lambda x : d_targets[x] < 2)].index)\n",
    "\n",
    "influencers = list(labels.groupby('u').count().index)\n",
    "targets = list(labels.groupby('v').count().index)\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))\n",
    "print(f'influencers : {len(influencers)}')\n",
    "print(f'targets : {len(targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_pickle(edges_path)\n",
    "\n",
    "d_edges = defaultdict(lambda : 0)\n",
    "for (u,v) in zip(edges.u, edges.v) :\n",
    "    d_edges[(u,v)] = 1\n",
    "del(edges)\n",
    "\n",
    "def feature_vector(u,v, fu=None, fv=None) : \n",
    "    \"\"\"\n",
    "    Creates vector with\n",
    "    - Influencers features\n",
    "    - Target features\n",
    "    - Topology link\n",
    "    \"\"\"\n",
    "    if fu is None or fv is None : \n",
    "        fu = features_influencers.loc[u]\n",
    "        fv = features_targets.loc[v]\n",
    "        \n",
    "    return np.concatenate([fu, fv, d_edges[(u,v)]], axis = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 6\n",
    "\n",
    "d_labels = defaultdict(lambda : False)\n",
    "for (u,v) in zip(labels.u, labels.v) :\n",
    "    d_labels[(u,v)] = True\n",
    "\n",
    "PROB_TYPE = 'JI'\n",
    "\n",
    "def fill_y(u,v) : \n",
    "    if d_labels[(u,v)] : \n",
    "        return labels.loc[(u,v)][PROB_TYPE]\n",
    "    else : \n",
    "        return 0\n",
    "\n",
    "@jit\n",
    "def softmax(x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "#@jit\n",
    "def create_XY(sampled_influencers, sampled_targets) :\n",
    "    \"\"\"\n",
    "    from 2 sets of influencers and targets, creates features and labels according to the paper format\n",
    "    \"\"\"\n",
    "    nI = len(sampled_influencers)\n",
    "    nT = len(sampled_targets)\n",
    "\n",
    "    X = np.zeros((nI, nT, N_FEATURES))\n",
    "    Y = np.zeros((nI, nT))\n",
    "    Y_emb = np.zeros((nI, nT))\n",
    "\n",
    "    #To not call loc for each (u,v)\n",
    "    fI = np.array(features_influencers.loc[sampled_influencers])\n",
    "    fT = np.array(features_targets.loc[sampled_targets])\n",
    "    eI = np.array(influencers_embeddings.loc[sampled_influencers])\n",
    "    eT = np.array(targets_embeddings.loc[sampled_targets])\n",
    "\n",
    "    for i in range(nI):\n",
    "        for j in range(nT):\n",
    "            u,v = sampled_influencers[i], sampled_targets[j]\n",
    "            X[i,j, :] = feature_vector(u, v, fI[i], fT[j])\n",
    "\n",
    "            #X[i,j, :] = np.concatenate([features_influencers.loc[sampled_influencers[i]], \n",
    "                                        # features_targets.loc[sampled_targets[j]]], \n",
    "                                        # axis = None)\n",
    "            Y[i,j] = fill_y(u,v)\n",
    "            Y_emb[i,j] = np.dot(eI[i], eT[j])\n",
    "        \n",
    "    Y = np.reshape(Y, (nI, nT,1))\n",
    "    \n",
    "    #transform each row into a probability distribution\n",
    "    Y_emb = np.reshape(Y_emb, (nI, nT, 1))\n",
    "    # Y_emb = np.apply_along_axis(lambda x:x-abs(max(x)), 1, Y_emb) \n",
    "    # Y_emb = np.apply_along_axis(softmax, 1, Y_emb)\n",
    "    # Y_emb = np.around(Y_emb,3)\n",
    "    # Y_emb = np.abs(Y_emb)\n",
    "\n",
    "    return np.concatenate((X, Y, Y_emb), axis = 2)\n",
    "\n",
    "# def fill_with_positive2(XY, p) :\n",
    "#     \"\"\"\n",
    "#     input : XY -> output of createXY, p -> proportion of positive examples needed in XY\n",
    "#     output : XY with the positive examples added\n",
    "#     \"\"\"\n",
    "#     nI, nT, _ = XY.shape\n",
    "#     n_pos = int(p * nI * nT)\n",
    "\n",
    "#     labels_to_add = labels.sample(n = n_pos)\n",
    "    \n",
    "#     for l in range(n_pos) :\n",
    "    \n",
    "#         i = np.random.randint(0,nI)\n",
    "#         t = np.random.randint(0, nT)\n",
    "    \n",
    "#         label = labels_to_add.iloc[l]\n",
    "#         f = feature_vector(label.u, label.v)\n",
    "#         #f = np.concatenate([features_influencers.loc[label.u], features_targets.loc[label.v]], axis = None)\n",
    "\n",
    "#         p_emb = np.dot(influencers_embeddings.loc[label.u], targets_embeddings.loc[label.v])\n",
    "\n",
    "#         XY[i, t, :] = np.concatenate([f, label[PROB_TYPE], p_emb], axis=None)\n",
    "    \n",
    "#     return XY\n",
    "\n",
    "def fill_with_positive(XY, p, sampled_influencers) :\n",
    "    \"\"\"\n",
    "    input : XY -> output of createXY, p -> proportion of positive examples needed in XY\n",
    "    output : XY with the positive examples added\n",
    "    \"\"\"\n",
    "    nI, nT, _ = XY.shape\n",
    "    \n",
    "    for i in range(nI) :\n",
    "        u = sampled_influencers[i]\n",
    "        pos_labels = labels[labels['u'] == u] #positive labels of the seed u\n",
    "        K = min(pos_labels.shape[0], int(p * nT)) \n",
    "        eI = influencers_embeddings.loc[u]\n",
    "\n",
    "        for k in range(K):\n",
    "            label = pos_labels.iloc[k]\n",
    "            f = feature_vector(u, label.v)\n",
    "            #f = np.concatenate([features_influencers.loc[label.u], features_targets.loc[label.v]], axis = None)\n",
    "            p_emb = np.dot(eI, targets_embeddings.loc[label.v])\n",
    "\n",
    "            XY[i, k, :] = np.concatenate([f, label[PROB_TYPE], p_emb], axis=None)\n",
    "    \n",
    "    return XY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only considering best P% influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROP_I = 0.2\n",
    "influencers = labels.groupby('u').count().sort_values('v', ascending=False)\n",
    "nI = influencers.shape[0]\n",
    "n = int(nI * PROP_I)\n",
    "influencers = influencers.iloc[:n].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_in         0.958646\n",
       "pagerank    14.351554\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_targets.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving instance 0/20...\n",
      "Saving instance 2/20...\n",
      "Saving instance 4/20...\n",
      "Saving instance 6/20...\n",
      "Saving instance 8/20...\n",
      "Saving instance 10/20...\n",
      "Saving instance 12/20...\n",
      "Saving instance 14/20...\n",
      "Saving instance 16/20...\n",
      "Saving instance 18/20...\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "path = 'decision_focused_learning_gpu/instances_weibo/06-23-all_200_twitter_500/'\n",
    "if not os.path.exists(path) :\n",
    "    os.mkdir(path)\n",
    "\n",
    "# PROP_POS = 0.\n",
    "N_INSTANCES = 20\n",
    "N_INFLUENCERS = 500\n",
    "N_TARGETS = 500\n",
    "\n",
    "for instance in range(N_INSTANCES) : \n",
    "\n",
    "    if instance % (N_INSTANCES // 10) == 0 : print(f\"Saving instance {instance}/{N_INSTANCES}...\")\n",
    "\n",
    "    if os.path.exists(path + f'{instance}.npz') :\n",
    "        print(\"Instance already created\")\n",
    "    else :\n",
    "        sampled_influencers = np.random.choice(influencers, N_INFLUENCERS, p = None, replace=False)\n",
    "        sampled_targets = np.random.choice(targets, N_TARGETS, p = None, replace=False)\n",
    "\n",
    "        XY = create_XY(sampled_influencers, sampled_targets)\n",
    "        # if PROP_POS > 0 :\n",
    "        #     XY = fill_with_positive(XY, PROP_POS, sampled_influencers)\n",
    "\n",
    "        np.savez(path + f'{instance}.npz', XY)    \n",
    "        del(XY)\n",
    "    \n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml-IM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7448610754f81a69301db522c2a53aaa2c29a9c8ef7a579a6927f76a50568678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
