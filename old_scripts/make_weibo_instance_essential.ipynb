{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_user = \"../data/weibo/userProfile.pkl\"\n",
    "file_labels = \"../data/weibo/labels_1000.pkl\"\n",
    "file_edges = \"../data/weibo/edges_1000.pkl\"\n",
    "\n",
    "DATE_VAR = ['created_at']\n",
    "NUM_VAR = ['bi_followers_count', 'followers_count', 'friends_count', 'statuses_count']\n",
    "CAT_VAR = ['city', 'verified', 'province', 'verified_type', 'gender']\n",
    "\n",
    "FEATURES = ['followers_count', 'friends_count', 'statuses_count', 'verified', 'gender']\n",
    "\n",
    "N_INFLUENCERS = 100\n",
    "N_TARGETS = 1000\n",
    "N_FEATURES = 2 * len(FEATURES)\n",
    "N_INSTANCES = 10\n",
    "\n",
    "PROB_TYPE = 'LP'\n",
    "PROP_POS = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_user_orig : \n",
      "|         id |         id |   bi_followers_count |   city | verified   |   followers_count |   province |   friends_count | gender   | created_at          |   verified_type |   statuses_count |\n",
      "|-----------:|-----------:|---------------------:|-------:|:-----------|------------------:|-----------:|----------------:|:---------|:--------------------|----------------:|-----------------:|\n",
      "| 1657151084 | 1657151084 |                    0 |      5 | False      |                33 |         31 |             162 | m        | 2009-10-29 22:20:41 |              -1 |                0 |\n",
      "| 1657149732 | 1657149732 |                    0 |      9 | False      |                16 |         62 |              27 | m        | 2009-10-29 20:48:01 |              -1 |               11 |\n",
      "| 1657148500 | 1657148500 |                    4 |     20 | False      |                79 |         31 |              29 | m        | 2009-12-12 18:10:42 |              -1 |               25 |\n",
      "| 1657146942 | 1657146942 |                    1 |   1000 | False      |                14 |         11 |              11 | m        | 2009-10-29 20:36:25 |              -1 |                3 |\n",
      "| 1657150271 | 1657150271 |                   73 |      1 | False      |               390 |         52 |             151 | f        | 2011-06-14 22:37:07 |             200 |               24 |\n",
      "shape : (1681085, 11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_orig = pd.read_pickle(file_user)\n",
    "df_user_orig.index = df_user_orig['id'].astype(np.int64)\n",
    "\n",
    "print(\"df_user_orig : \\n\" + df_user_orig.head(5).to_markdown())\n",
    "print(f\"shape : {df_user_orig.shape}\\n\" )\n",
    "\n",
    "#FE\n",
    "df_user = df_user_orig[FEATURES]\n",
    "df_user['followers_count'] = df_user['followers_count'].apply(lambda x : np.log(max(x, 0) + 1)) / 10\n",
    "df_user['friends_count'] = df_user['friends_count'].apply(lambda x : np.log(max(x, 0) + 1)) / 8\n",
    "df_user['statuses_count'] = df_user['statuses_count'].apply(lambda x : np.log(max(x, 0) + 1)) / 10\n",
    "df_user['verified'] = df_user.verified.apply(lambda x : 1 if x =='True' else 0)\n",
    "df_user['gender'] = df_user.gender.cat.codes\n",
    "df_user = df_user.groupby(df_user.index).max() #some userid were 2 time in the dataset 1681085 rows -> 1655678 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of ground truths previously estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                      |               u |               v |   BT |   JI |   LP |\n",
      "|:---------------------|----------------:|----------------:|-----:|-----:|-----:|\n",
      "| (82768, 82768)       | 82768           | 82768           |  1   | 0.5  |  1   |\n",
      "| (7747002, 7747002)   |     7.747e+06   |     7.747e+06   |  1   | 0.5  |  1   |\n",
      "| (8060099, 8060099)   |     8.0601e+06  |     8.0601e+06  |  1   | 0.5  |  1   |\n",
      "| (15058618, 3023198)  |     1.50586e+07 |     3.0232e+06  |  1   | 0.5  |  1   |\n",
      "| (32821757, 32821757) |     3.28218e+07 |     3.28218e+07 |  0.5 | 0.25 |  0.5 |\n",
      "shape : (147693, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_pickle(file_labels)\n",
    "labels.u = labels.u.astype(np.int64)\n",
    "labels.v = labels.v.astype(np.int64)\n",
    "labels.index = pd.MultiIndex.from_tuples(zip(labels['u'],labels['v'])) #important to do .loc[(u,v)]\n",
    "labels = labels.sort_index() # infos are retreived faster\n",
    "labels = labels.drop_duplicates()\n",
    "labels = labels.drop((1637712471, 279405)) #1637712471 is not present in df_user_num (why ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total influencers : 823\n",
      "Total targets : 133678\n"
     ]
    }
   ],
   "source": [
    "influencers = list(labels.groupby('u').count().index)\n",
    "targets = list(labels.groupby('v').count().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_y(u,v) : \n",
    "    if (u,v) in labels.index : \n",
    "        return labels.loc[(u,v)][PROB_TYPE]\n",
    "    else : \n",
    "        return 0\n",
    "\n",
    "def create_XY(sampled_influencers, sampled_targets) :\n",
    "    \"\"\"\n",
    "    from 2 sets of influencers and targets, creates features and labels according to the paper format\n",
    "    \"\"\"\n",
    "    nI = len(sampled_influencers)\n",
    "    nT = len(sampled_targets)\n",
    "    X = np.zeros((nI, nT, N_FEATURES))\n",
    "\n",
    "    for target in range(nT):\n",
    "        X[:, target, :] = np.c_[np.array(df_user.loc[sampled_influencers]), np.tile(df_user.loc[sampled_targets[target]],(nI, 1))]\n",
    "\n",
    "    Y = np.zeros((nI, nT))\n",
    "\n",
    "    for i in range(nI):\n",
    "        for j in range(nT):\n",
    "            Y[i,j] = fill_y(sampled_influencers[i], sampled_targets[j])\n",
    "\n",
    "    Y = np.reshape(Y, (nI, nT,1))\n",
    "\n",
    "    return np.concatenate((X,Y), axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_with_positive(XY, p) :\n",
    "    \"\"\"\n",
    "    input : XY -> output of createXY, p -> proportion of positive examples needed in XY\n",
    "    output : XY with the positive examples added\n",
    "    \"\"\"\n",
    "    nI, nT, _ = XY.shape\n",
    "    n_pos = int(p * nI * nT)\n",
    "\n",
    "    labels_to_add = labels.sample(n = n_pos)\n",
    "    for l in range(n_pos) :\n",
    "    \n",
    "        i = np.random.randint(0,nI)\n",
    "        t = np.random.randint(0, nT)\n",
    "    \n",
    "        label = labels_to_add.iloc[l]\n",
    "        fu = df_user.loc[label.u]\n",
    "        fv = df_user.loc[label.v]\n",
    "    \n",
    "        XY[i, t, :] = np.concatenate([fu, fv, label[PROB_TYPE]], axis=None)\n",
    "    \n",
    "    return XY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbu = labels.groupby('u').count().v\n",
    "influencers = list(gbu.index)\n",
    "p_influencers = list(gbu.values / sum(gbu)) \n",
    "del(gbu)\n",
    "\n",
    "gbv = labels.groupby('v').count().u\n",
    "targets = list(gbv.index)\n",
    "p_targets = list(gbv.values / sum(gbv))\n",
    "del(gbv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving instance 0/10...\n",
      "Saving instance 1/10...\n",
      "Saving instance 2/10...\n",
      "Saving instance 3/10...\n",
      "Saving instance 4/10...\n",
      "Saving instance 5/10...\n",
      "Saving instance 6/10...\n",
      "Saving instance 7/10...\n",
      "Saving instance 8/10...\n",
      "Saving instance 9/10...\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "path = '../decision_focused_learning_gpu/instances_weibo/oversampled_FE_LP/'\n",
    "\n",
    "for instance in range(N_INSTANCES) : \n",
    "\n",
    "    if instance % (N_INSTANCES // 10) == 0 : print(f\"Saving instance {instance}/{N_INSTANCES}...\")\n",
    "\n",
    "    if os.path.exists(path + f'{instance}.npz') :\n",
    "        print(\"Instance already created\")\n",
    "    else :\n",
    "        sampled_influencers = np.random.choice(influencers, N_INFLUENCERS, p = p_influencers, replace=False)\n",
    "        sampled_targets = np.random.choice(targets, N_TARGETS, p = p_targets, replace=False)\n",
    "\n",
    "        XY = create_XY(sampled_influencers, sampled_targets)\n",
    "        XY = fill_with_positive(XY, PROP_POS)\n",
    "\n",
    "        np.savez(path + f'{instance}.npz', XY)    \n",
    "        del(XY)\n",
    "    \n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the topology edges info to instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = pd.read_pickle(file_edges)\n",
    "def f(): return False\n",
    "d_edges = defaultdict(f)\n",
    "for (u,v) in zip(edges.u, edges.v) :\n",
    "    d_edges[(u,v)] = True\n",
    "del(edges)\n",
    "\n",
    "def feature_vector(u,v) : \n",
    "\n",
    "    fu = df_user.loc[u]\n",
    "    fv = df_user.loc[v]\n",
    "\n",
    "    \n",
    "    XY[i, t, :] = np.concatenate([fu, fv, label[PROB_TYPE]], axis=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc0f5465fe995d1bcad0eb0cccd8f49aa4a13151dec79867121107c7f441e047"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml-IM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
