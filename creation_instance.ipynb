{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_influencers_path = \"data/weibo_features/features_influencers_100K_150_emb.pkl\"\n",
    "features_targets_path = \"data/weibo_features/features_targets_100K_150_emb.pkl\"\n",
    "labels_path = \"data/weibo_preprocessed/labels2_100K_150.pkl\"\n",
    "edges_path = \"data/weibo_preprocessed/edges2_100K_150.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_influencers : \n",
      "0     float64\n",
      "1     float64\n",
      "2     float64\n",
      "3     float64\n",
      "4     float64\n",
      "5     float64\n",
      "6     float64\n",
      "7     float64\n",
      "8     float64\n",
      "9     float64\n",
      "10    float64\n",
      "11    float64\n",
      "12    float64\n",
      "13    float64\n",
      "14    float64\n",
      "15    float64\n",
      "16    float64\n",
      "17    float64\n",
      "18    float64\n",
      "19    float64\n",
      "20    float64\n",
      "21    float64\n",
      "22    float64\n",
      "23    float64\n",
      "24    float64\n",
      "25    float64\n",
      "26    float64\n",
      "27    float64\n",
      "28    float64\n",
      "29    float64\n",
      "30    float64\n",
      "31    float64\n",
      "32    float64\n",
      "33    float64\n",
      "34    float64\n",
      "35    float64\n",
      "36    float64\n",
      "37    float64\n",
      "38    float64\n",
      "39    float64\n",
      "40    float64\n",
      "41    float64\n",
      "42    float64\n",
      "43    float64\n",
      "44    float64\n",
      "45    float64\n",
      "46    float64\n",
      "47    float64\n",
      "48    float64\n",
      "49    float64\n",
      "dtype: object\n",
      "shape : (6417, 50)\n",
      "features_targets : \n",
      "0     float64\n",
      "1     float64\n",
      "2     float64\n",
      "3     float64\n",
      "4     float64\n",
      "5     float64\n",
      "6     float64\n",
      "7     float64\n",
      "8     float64\n",
      "9     float64\n",
      "10    float64\n",
      "11    float64\n",
      "12    float64\n",
      "13    float64\n",
      "14    float64\n",
      "15    float64\n",
      "16    float64\n",
      "17    float64\n",
      "18    float64\n",
      "19    float64\n",
      "20    float64\n",
      "21    float64\n",
      "22    float64\n",
      "23    float64\n",
      "24    float64\n",
      "25    float64\n",
      "26    float64\n",
      "27    float64\n",
      "28    float64\n",
      "29    float64\n",
      "30    float64\n",
      "31    float64\n",
      "32    float64\n",
      "33    float64\n",
      "34    float64\n",
      "35    float64\n",
      "36    float64\n",
      "37    float64\n",
      "38    float64\n",
      "39    float64\n",
      "40    float64\n",
      "41    float64\n",
      "42    float64\n",
      "43    float64\n",
      "44    float64\n",
      "45    float64\n",
      "46    float64\n",
      "47    float64\n",
      "48    float64\n",
      "49    float64\n",
      "dtype: object\n",
      "shape : (3942, 50)\n"
     ]
    }
   ],
   "source": [
    "features_influencers = pd.read_pickle(features_influencers_path)\n",
    "features_targets = pd.read_pickle(features_targets_path)\n",
    "print(\"features_influencers : \\n\" + str(features_influencers.dtypes))\n",
    "print(\"shape : \" + str(features_influencers.shape))\n",
    "print(\"features_targets : \\n\" + str(features_targets.dtypes))\n",
    "print(\"shape : \" + str(features_targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                  |      u |      v |   BT |        JI |        LP |\n",
      "|:-----------------|-------:|-------:|-----:|----------:|----------:|\n",
      "| (41499, 41499)   |  41499 |  41499 |    1 | 0.0277778 | 0.0285714 |\n",
      "| (101713, 101713) | 101713 | 101713 |    1 | 0.0135135 | 0.0136986 |\n",
      "shape : (179932, 5)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_pickle(labels_path)\n",
    "labels.index = pd.MultiIndex.from_tuples(zip(labels['u'],labels['v'])) #important to do .loc[(u,v)]\n",
    "labels = labels.sort_index() # infos are retreived faster\n",
    "labels = labels.drop_duplicates()\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing the labels where we do not have the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                      |      u |                v |   BT |         JI |         LP |\n",
      "|:---------------------|-------:|-----------------:|-----:|-----------:|-----------:|\n",
      "| (101713, 101713)     | 101713 | 101713           |    1 | 0.0135135  | 0.0136986  |\n",
      "| (200880, 1840240511) | 200880 |      1.84024e+09 |    1 | 0.00917431 | 0.00925926 |\n",
      "shape : (164066, 5)\n"
     ]
    }
   ],
   "source": [
    "influencers = list(features_influencers.index)\n",
    "d_influencers = defaultdict(lambda : False)\n",
    "for i in influencers : d_influencers[i] = True\n",
    "\n",
    "targets = list(features_targets.index)\n",
    "d_targets = defaultdict(lambda : False)\n",
    "for i in targets : d_targets[i]=True\n",
    "\n",
    "labels = labels.drop(labels[labels.u.apply(lambda x : not d_influencers[x])].index)\n",
    "labels = labels.drop(labels[labels.v.apply(lambda x : not d_targets[x])].index)\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_pickle(edges_path)\n",
    "\n",
    "def f(): return 0\n",
    "d_edges = defaultdict(f)\n",
    "for (u,v) in zip(edges.u, edges.v) :\n",
    "    d_edges[(u,v)] = 1\n",
    "del(edges)\n",
    "\n",
    "def feature_vector(u,v) : \n",
    "    \"\"\"\n",
    "    Creates vector with\n",
    "    - Influencers features\n",
    "    - Target features\n",
    "    - Topology link\n",
    "    \"\"\"\n",
    "    fu = features_influencers.loc[u]\n",
    "    fv = features_targets.loc[v]\n",
    "    \n",
    "    return np.concatenate([fu, fv, d_edges[(u,v)]], axis = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_FEATURES = 100\n",
    "\n",
    "d_labels = defaultdict(lambda : False)\n",
    "for (u,v) in zip(labels.u, labels.v) :\n",
    "    d_labels[(u,v)] = True\n",
    "\n",
    "PROB_TYPE = 'LP'\n",
    "\n",
    "def fill_y(u,v) : \n",
    "    if d_labels[(u,v)] : \n",
    "        return labels.loc[(u,v)][PROB_TYPE]\n",
    "    else : \n",
    "        return 0\n",
    "@jit\n",
    "def create_XY(sampled_influencers, sampled_targets) :\n",
    "    \"\"\"\n",
    "    from 2 sets of influencers and targets, creates features and labels according to the paper format\n",
    "    \"\"\"\n",
    "    nI = len(sampled_influencers)\n",
    "    nT = len(sampled_targets)\n",
    "    X = np.zeros((nI, nT, N_FEATURES))\n",
    "\n",
    "    Y = np.zeros((nI, nT))\n",
    "\n",
    "    for i in range(nI):\n",
    "        for j in range(nT):\n",
    "            #X[i,j, :] = feature_vector(sampled_influencers[i], sampled_targets[j])\n",
    "            X[i,j, :] = np.concatenate([features_influencers.loc[sampled_influencers[i]], features_targets.loc[sampled_targets[j]]], axis = None)\n",
    "            Y[i,j] = fill_y(sampled_influencers[i], sampled_targets[j])\n",
    "\n",
    "    Y = np.reshape(Y, (nI, nT,1))\n",
    "\n",
    "    return np.concatenate((X,Y), axis = 2)\n",
    "\n",
    "def fill_with_positive(XY, p) :\n",
    "    \"\"\"\n",
    "    input : XY -> output of createXY, p -> proportion of positive examples needed in XY\n",
    "    output : XY with the positive examples added\n",
    "    \"\"\"\n",
    "    nI, nT, _ = XY.shape\n",
    "    n_pos = int(p * nI * nT)\n",
    "\n",
    "    labels_to_add = labels.sample(n = n_pos)\n",
    "    labels\n",
    "    for l in range(n_pos) :\n",
    "    \n",
    "        i = np.random.randint(0,nI)\n",
    "        t = np.random.randint(0, nT)\n",
    "    \n",
    "        label = labels_to_add.iloc[l]\n",
    "        #f = feature_vector(label.u, label.v)\n",
    "        f = np.concatenate([features_influencers.loc[label.u], features_targets.loc[label.v]], axis = None)\n",
    "\n",
    "        XY[i, t, :] = np.concatenate([f, label[PROB_TYPE]], axis=None)\n",
    "    \n",
    "    return XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving instance 0/10...\n",
      "Saving instance 1/10...\n",
      "Saving instance 2/10...\n",
      "Saving instance 3/10...\n",
      "Saving instance 4/10...\n",
      "Saving instance 5/10...\n",
      "Saving instance 6/10...\n",
      "Saving instance 7/10...\n",
      "Saving instance 8/10...\n",
      "Saving instance 9/10...\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "path = 'decision_focused_learning_gpu/instances_weibo/embeddings/'\n",
    "\n",
    "PROP_POS = 0.2\n",
    "N_INSTANCES = 10\n",
    "N_INFLUENCERS = 500\n",
    "N_TARGETS = 500\n",
    "\n",
    "for instance in range(N_INSTANCES) : \n",
    "\n",
    "    if instance % (N_INSTANCES // 10) == 0 : print(f\"Saving instance {instance}/{N_INSTANCES}...\")\n",
    "\n",
    "    if os.path.exists(path + f'{instance}.npz') :\n",
    "        print(\"Instance already created\")\n",
    "    else :\n",
    "        sampled_influencers = np.random.choice(influencers, N_INFLUENCERS, p = None, replace=False)\n",
    "        sampled_targets = np.random.choice(targets, N_TARGETS, p = None, replace=False)\n",
    "\n",
    "        XY = create_XY(sampled_influencers, sampled_targets)\n",
    "        if PROP_POS > 0 :\n",
    "            XY = fill_with_positive(XY, PROP_POS)\n",
    "\n",
    "        np.savez(path + f'{instance}.npz', XY)    \n",
    "        del(XY)\n",
    "    \n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7ea4e65ffb6255b684281256f6acd9d3a63de41c35d542089cc796fcc4db3b9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('greedy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
