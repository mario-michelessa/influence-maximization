{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_influencers_path = \"data/weibo_features/features_influencers_100K_150.pkl\"\n",
    "features_targets_path = \"data/weibo_features/features_targets_100K_150.pkl\"\n",
    "labels_path = \"data/weibo_preprocessed/labels2_100K_150.pkl\"\n",
    "edges_path = \"data/weibo_preprocessed/edges2_100K_150.pkl\"\n",
    "influencers_embeddings_path = \"data/weibo_preprocessed/influencers_embeddings.pkl\"\n",
    "targets_embeddings_path = \"data/weibo_preprocessed/target_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_influencers : \n",
      "followers_count    float64\n",
      "friends_count      float64\n",
      "statuses_count     float64\n",
      "verified              int8\n",
      "gender                int8\n",
      "d_out              float64\n",
      "0                  float64\n",
      "1                  float64\n",
      "2                  float64\n",
      "3                  float64\n",
      "4                  float64\n",
      "5                  float64\n",
      "6                  float64\n",
      "7                  float64\n",
      "dtype: object\n",
      "shape : (8465, 14)\n",
      "features_targets : \n",
      "followers_count    float64\n",
      "friends_count      float64\n",
      "statuses_count     float64\n",
      "verified              int8\n",
      "gender                int8\n",
      "d_in               float64\n",
      "0                  float64\n",
      "1                  float64\n",
      "2                  float64\n",
      "3                  float64\n",
      "4                  float64\n",
      "5                  float64\n",
      "6                  float64\n",
      "7                  float64\n",
      "dtype: object\n",
      "shape : (3976, 14)\n"
     ]
    }
   ],
   "source": [
    "features_influencers = pd.read_pickle(features_influencers_path)\n",
    "features_targets = pd.read_pickle(features_targets_path)\n",
    "print(\"features_influencers : \\n\" + str(features_influencers.dtypes))\n",
    "print(\"shape : \" + str(features_influencers.shape))\n",
    "print(\"features_targets : \\n\" + str(features_targets.dtypes))\n",
    "print(\"shape : \" + str(features_targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                  |      u |      v |   BT |        JI |        LP |\n",
      "|:-----------------|-------:|-------:|-----:|----------:|----------:|\n",
      "| (41499, 41499)   |  41499 |  41499 |    1 | 0.0277778 | 0.0285714 |\n",
      "| (101713, 101713) | 101713 | 101713 |    1 | 0.0135135 | 0.0136986 |\n",
      "shape : (179932, 5)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_pickle(labels_path)\n",
    "labels.index = pd.MultiIndex.from_tuples(zip(labels['u'],labels['v'])) #important to do .loc[(u,v)]\n",
    "labels = labels.sort_index() # infos are retreived faster\n",
    "labels = labels.drop_duplicates()\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))\n",
    "\n",
    "influencers_embeddings = pd.read_pickle(influencers_embeddings_path)\n",
    "targets_embeddings = pd.read_pickle(targets_embeddings_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : \n",
      "|                      |      u |                v |   BT |         JI |         LP |\n",
      "|:---------------------|-------:|-----------------:|-----:|-----------:|-----------:|\n",
      "| (101713, 101713)     | 101713 | 101713           |    1 | 0.0135135  | 0.0136986  |\n",
      "| (200880, 1840240511) | 200880 |      1.84024e+09 |    1 | 0.00917431 | 0.00925926 |\n",
      "shape : (164026, 5)\n",
      "influencers : 6406\n",
      "targets : 3937\n"
     ]
    }
   ],
   "source": [
    "# removing the labels where we do not have the embeddings\n",
    "# and removing the labels where we do not have the features\n",
    "\n",
    "d_influencers = defaultdict(lambda : 0)\n",
    "for i in list(influencers_embeddings.index) : d_influencers[i] += 1\n",
    "for i in list(features_influencers.index) : d_influencers[i] += 1\n",
    "\n",
    "d_targets = defaultdict(lambda : 0)\n",
    "for i in list(targets_embeddings.index) : d_targets[i] += 1\n",
    "for i in list(features_targets.index) : d_targets[i] += 1\n",
    "\n",
    "labels = labels.drop(labels[labels.u.apply(lambda x : d_influencers[x] < 2)].index)\n",
    "labels = labels.drop(labels[labels.v.apply(lambda x : d_targets[x] < 2)].index)\n",
    "\n",
    "influencers = list(labels.groupby('u').count().index)\n",
    "targets = list(labels.groupby('v').count().index)\n",
    "\n",
    "print(\"labels : \\n\" + labels.head(2).to_markdown())\n",
    "print(\"shape : \" + str(labels.shape))\n",
    "print(f'influencers : {len(influencers)}')\n",
    "print(f'targets : {len(targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_pickle(edges_path)\n",
    "\n",
    "d_edges = defaultdict(lambda : 0)\n",
    "for (u,v) in zip(edges.u, edges.v) :\n",
    "    d_edges[(u,v)] = 1\n",
    "del(edges)\n",
    "\n",
    "def feature_vector(u,v, fu=None, fv=None) : \n",
    "    \"\"\"\n",
    "    Creates vector with\n",
    "    - Influencers features\n",
    "    - Target features\n",
    "    - Topology link\n",
    "    \"\"\"\n",
    "    if fu is None or fv is None : \n",
    "        fu = features_influencers.loc[u]\n",
    "        fv = features_targets.loc[v]\n",
    "        \n",
    "    return np.concatenate([fu, fv, d_edges[(u,v)]], axis = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_FEATURES = 29\n",
    "\n",
    "d_labels = defaultdict(lambda : False)\n",
    "for (u,v) in zip(labels.u, labels.v) :\n",
    "    d_labels[(u,v)] = True\n",
    "\n",
    "PROB_TYPE = 'JI'\n",
    "\n",
    "def fill_y(u,v) : \n",
    "    if d_labels[(u,v)] : \n",
    "        return labels.loc[(u,v)][PROB_TYPE]\n",
    "    else : \n",
    "        return 0\n",
    "\n",
    "@jit\n",
    "def softmax(x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "#@jit\n",
    "def create_XY(sampled_influencers, sampled_targets) :\n",
    "    \"\"\"\n",
    "    from 2 sets of influencers and targets, creates features and labels according to the paper format\n",
    "    \"\"\"\n",
    "    nI = len(sampled_influencers)\n",
    "    nT = len(sampled_targets)\n",
    "\n",
    "    X = np.zeros((nI, nT, N_FEATURES))\n",
    "    Y = np.zeros((nI, nT))\n",
    "    Y_emb = np.zeros((nI, nT))\n",
    "\n",
    "    #To not call loc for each (u,v)\n",
    "    fI = np.array(features_influencers.loc[sampled_influencers])\n",
    "    fT = np.array(features_targets.loc[sampled_targets])\n",
    "    eI = np.array(influencers_embeddings.loc[sampled_influencers])\n",
    "    eT = np.array(targets_embeddings.loc[sampled_targets])\n",
    "\n",
    "    for i in range(nI):\n",
    "        for j in range(nT):\n",
    "            u,v = sampled_influencers[i], sampled_targets[j]\n",
    "            X[i,j, :] = feature_vector(u, v, fI[i], fT[j])\n",
    "\n",
    "            #X[i,j, :] = np.concatenate([features_influencers.loc[sampled_influencers[i]], \n",
    "                                        # features_targets.loc[sampled_targets[j]]], \n",
    "                                        # axis = None)\n",
    "            Y[i,j] = fill_y(u,v)\n",
    "            Y_emb[i,j] = np.dot(eI[i], eT[j])\n",
    "        \n",
    "    Y = np.reshape(Y, (nI, nT,1))\n",
    "    \n",
    "    #transform each row into a probability distribution\n",
    "    Y_emb = np.reshape(Y_emb, (nI, nT, 1))\n",
    "    Y_emb = np.apply_along_axis(lambda x:x-abs(max(x)), 1, Y_emb) \n",
    "    Y_emb = np.apply_along_axis(softmax, 1, Y_emb)\n",
    "    Y_emb = np.around(Y_emb,3)\n",
    "    Y_emb = np.abs(Y_emb)\n",
    "\n",
    "    return np.concatenate((X, Y, Y_emb), axis = 2)\n",
    "\n",
    "def fill_with_positive(XY, p) :\n",
    "    \"\"\"\n",
    "    input : XY -> output of createXY, p -> proportion of positive examples needed in XY\n",
    "    output : XY with the positive examples added\n",
    "    \"\"\"\n",
    "    nI, nT, _ = XY.shape\n",
    "    n_pos = int(p * nI * nT)\n",
    "\n",
    "    labels_to_add = labels.sample(n = n_pos)\n",
    "    labels\n",
    "    for l in range(n_pos) :\n",
    "    \n",
    "        i = np.random.randint(0,nI)\n",
    "        t = np.random.randint(0, nT)\n",
    "    \n",
    "        label = labels_to_add.iloc[l]\n",
    "        f = feature_vector(label.u, label.v)\n",
    "        #f = np.concatenate([features_influencers.loc[label.u], features_targets.loc[label.v]], axis = None)\n",
    "\n",
    "        p_emb = np.dot(influencers_embeddings.loc[label.u], targets_embeddings.loc[label.v])\n",
    "\n",
    "        XY[i, t, :] = np.concatenate([f, label[PROB_TYPE], p_emb], axis=None)\n",
    "    \n",
    "    return XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving instance 0/10...\n",
      "Saving instance 1/10...\n",
      "Saving instance 2/10...\n",
      "Saving instance 3/10...\n",
      "Saving instance 4/10...\n",
      "Saving instance 5/10...\n",
      "Saving instance 6/10...\n",
      "Saving instance 7/10...\n",
      "Saving instance 8/10...\n",
      "Saving instance 9/10...\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "path = 'decision_focused_learning_gpu/instances_weibo/05-05-embeddings_labels/'\n",
    "if not os.path.exists(path) :\n",
    "    os.mkdir(path)\n",
    "\n",
    "PROP_POS = 0\n",
    "N_INSTANCES = 10\n",
    "N_INFLUENCERS = 500\n",
    "N_TARGETS = 500\n",
    "\n",
    "for instance in range(N_INSTANCES) : \n",
    "\n",
    "    if instance % (N_INSTANCES // 10) == 0 : print(f\"Saving instance {instance}/{N_INSTANCES}...\")\n",
    "\n",
    "    if os.path.exists(path + f'{instance}.npz') :\n",
    "        print(\"Instance already created\")\n",
    "    else :\n",
    "        sampled_influencers = np.random.choice(influencers, N_INFLUENCERS, p = None, replace=False)\n",
    "        sampled_targets = np.random.choice(targets, N_TARGETS, p = None, replace=False)\n",
    "\n",
    "        XY = create_XY(sampled_influencers, sampled_targets)\n",
    "        if PROP_POS > 0 :\n",
    "            XY = fill_with_positive(XY, PROP_POS)\n",
    "\n",
    "        np.savez(path + f'{instance}.npz', XY)    \n",
    "        del(XY)\n",
    "    \n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7ea4e65ffb6255b684281256f6acd9d3a63de41c35d542089cc796fcc4db3b9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('greedy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
